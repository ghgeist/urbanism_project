{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely import wkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\\WalkabilityIndex\\Natl_WI_simplified.csv using pandas\n",
    "df = pd.read_csv(r'data\\WalkabilityIndex\\Natl_WI_simplified.csv')\n",
    "\n",
    "# Convert WKT geometries to actual geometries\n",
    "df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "\n",
    "# Create a GeoDataFrame with the initial CRS (assuming EPSG:4326)\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry', crs='EPSG:4326')\n",
    "\n",
    "# Display the first row\n",
    "gdf.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to figure out how unproductive land works in the data set\n",
    "# Calculate the percent of unproductive land by total acres\n",
    "gdf['percent_unproductive'] = round(gdf['ac_unpr'] / gdf['ac_total'] * 100, 2)\n",
    "\n",
    "# Create bins for the percent_unproductive values\n",
    "bins = range(0, 100, 10)\n",
    "gdf['bins'] = pd.cut(gdf['percent_unproductive'], bins=bins)\n",
    "\n",
    "# Group by the bins and count the geoid20s in each bin\n",
    "bin_counts = gdf.groupby('bins', observed=False)['geoid20'].count()\n",
    "\n",
    "# Convert the Series to a DataFrame\n",
    "bin_counts_df = bin_counts.reset_index()\n",
    "bin_counts_df.columns = ['bin', 'geoid20_count']\n",
    "\n",
    "# Calculate the total count of geoid20\n",
    "total_count = bin_counts_df['geoid20_count'].sum()\n",
    "\n",
    "# Calculate the percentage for each bin\n",
    "bin_counts_df['percentage'] = (bin_counts_df['geoid20_count'] / total_count) * 100\n",
    "\n",
    "# Print the DataFrame\n",
    "bin_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cbsa_name</th>\n",
       "      <th>geoid20_count</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>Los Angeles-Long Beach-Anaheim, CA</td>\n",
       "      <td>8248</td>\n",
       "      <td>6.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              cbsa_name  geoid20_count  percentage\n",
       "291  Los Angeles-Long Beach-Anaheim, CA           8248        6.81"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the geoid20 per cbsa_name\n",
    "cbsa_counts = gdf.groupby('cbsa_name')['geoid20'].count()\n",
    "\n",
    "# Convert the Series to a DataFrame\n",
    "cbsa_counts_df = cbsa_counts.reset_index()\n",
    "cbsa_counts_df.columns = ['cbsa_name', 'geoid20_count']\n",
    "\n",
    "# Calculate the total count of geoid20\n",
    "total_geoid20_count = cbsa_counts_df['geoid20_count'].sum()\n",
    "\n",
    "# Calculate the percentage for each cbsa_name\n",
    "cbsa_counts_df['percentage'] = round((cbsa_counts_df['geoid20_count'] / total_geoid20_count) * 100, 2)\n",
    "\n",
    "# Sort the DataFrame by geoid20_count in descending order\n",
    "cbsa_counts_df.sort_values('geoid20_count', ascending=False, inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "cbsa_counts_df.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of geoid20 where cbsa_name is null: 10178\n",
      "Total number of rows in the DataFrame: 131264\n",
      "Percentage of rows where cbsa_name is null: 7.75%\n"
     ]
    }
   ],
   "source": [
    "# Trying to figure out if it makes sense to drop the cases where the CBSA name is null\n",
    "# Count the geoid20 where cbsa_name is null\n",
    "null_cbsa_count = gdf[gdf['cbsa_name'].isnull()]['geoid20'].count()\n",
    "print(f'Number of geoid20 where cbsa_name is null: {null_cbsa_count}')\n",
    "\n",
    "# Count the total number of rows in the DataFrame\n",
    "total_rows = len(gdf)\n",
    "print(f'Total number of rows in the DataFrame: {total_rows}')\n",
    "\n",
    "# Calculate the percentage of rows where cbsa_name is null\n",
    "null_cbsa_percentage = (null_cbsa_count / total_rows) * 100\n",
    "print(f'Percentage of rows where cbsa_name is null: {null_cbsa_percentage:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.312640e+05\n",
       "mean     1.124656e+04\n",
       "std      1.944862e+05\n",
       "min      1.096136e+00\n",
       "25%      1.290690e+02\n",
       "50%      3.246184e+02\n",
       "75%      1.996697e+03\n",
       "max      4.055649e+07\n",
       "Name: ac_total, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking to understand the distribution of the ac_total column\n",
    "gdf['ac_total'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large Variability: The data exhibits a wide range of area sizes, from just over 1 acre to over 40 million acres. The large standard deviation and high maximum value indicate that while most areas are relatively small to moderate in size, there are a few extremely large areas that are significantly larger than the rest.\n",
    "\n",
    "Skewed Distribution: The mean being much higher than the median suggests a right-skewed distribution, where the presence of a few very large values is pulling the average up. This is further evidenced by the 75th percentile being well below the mean.\n",
    "\n",
    "Outliers: The maximum value of over 40 million acres is an outlier, and it could significantly affect the mean and standard deviation. Depending on the analysis, it might be important to consider how this outlier influences overall results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
